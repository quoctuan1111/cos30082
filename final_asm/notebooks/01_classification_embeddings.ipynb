{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bab0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: imports and configuration \n",
    "from pathlib import Path\n",
    "import os, random, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device (CPU only)\n",
    "DEVICE = \"cpu\"\n",
    "print(\"Running on:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5515e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\study\\cos30082\\final_asm\n",
      "Data: True\n",
      "Models folder: True\n",
      "Artifacts folder: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: project paths\n",
    "NB_DIR = Path.cwd()              \n",
    "PROJ_ROOT = NB_DIR.parent         \n",
    "\n",
    "DATA_DIR = PROJ_ROOT / \"data\"\n",
    "CLASS_DIR = DATA_DIR / \"classification_data\"\n",
    "TRAIN_DIR = CLASS_DIR / \"train_data\"\n",
    "VAL_DIR   = CLASS_DIR / \"val_data\"\n",
    "TEST_DIR  = CLASS_DIR / \"test_data\"\n",
    "\n",
    "MODEL_DIR = PROJ_ROOT / \"models\"\n",
    "ARTIFACT_DIR = PROJ_ROOT / \"artifacts\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJ_ROOT)\n",
    "print(\"Data:\", DATA_DIR.exists())\n",
    "print(\"Models folder:\", MODEL_DIR.exists())\n",
    "print(\"Artifacts folder:\", ARTIFACT_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438780a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 4000\n",
      "Train images: 380638\n",
      "Val images: 8000\n"
     ]
    }
   ],
   "source": [
    "#  Cell 3: data transforms and loaders\n",
    "IMG_SIZE = 160\n",
    "BATCH_SIZE = 64  # moderate batch size for CPU\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(VAL_DIR,   transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "num_classes = len(train_ds.classes)\n",
    "print(f\"Classes: {num_classes}\")\n",
    "print(f\"Train images: {len(train_ds)}\")\n",
    "print(f\"Val images: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc798daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([3, 160, 160])\n",
      "Label index: 0 -> n000003\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: verify one sample\n",
    "img, label = train_ds[0]\n",
    "print(\"Image tensor shape:\", img.shape)\n",
    "print(\"Label index:\", label, \"->\", train_ds.classes[label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Model architecture (pretrained + fine-tuning)\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class FaceNetSoftmax(nn.Module):\n",
    "    def __init__(self, num_classes, emb_dim=128):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        feat_dim = backbone.fc.in_features\n",
    "        # Fine-tune only higher layers for speed/accuracy trade-off\n",
    "        for name, param in backbone.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if \"layer4\" in name or \"bn1\" in name or \"conv1\" in name:\n",
    "                param.requires_grad = True\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.embed = nn.Linear(feat_dim, emb_dim)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        f = self.backbone(x)\n",
    "        e = nn.functional.normalize(self.embed(f))\n",
    "        logits = self.classifier(e)\n",
    "        probs = self.softmax(logits)\n",
    "        if return_embedding:\n",
    "            return e\n",
    "        return logits, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35aca174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous weights (added Softmax for compliance)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 80, 80]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 80, 80]             128\n",
      "              ReLU-3           [-1, 64, 80, 80]               0\n",
      "         MaxPool2d-4           [-1, 64, 40, 40]               0\n",
      "            Conv2d-5           [-1, 64, 40, 40]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 40, 40]             128\n",
      "              ReLU-7           [-1, 64, 40, 40]               0\n",
      "            Conv2d-8           [-1, 64, 40, 40]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 40, 40]             128\n",
      "             ReLU-10           [-1, 64, 40, 40]               0\n",
      "       BasicBlock-11           [-1, 64, 40, 40]               0\n",
      "           Conv2d-12           [-1, 64, 40, 40]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 40, 40]             128\n",
      "             ReLU-14           [-1, 64, 40, 40]               0\n",
      "           Conv2d-15           [-1, 64, 40, 40]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 40, 40]             128\n",
      "             ReLU-17           [-1, 64, 40, 40]               0\n",
      "       BasicBlock-18           [-1, 64, 40, 40]               0\n",
      "           Conv2d-19          [-1, 128, 20, 20]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 20, 20]             256\n",
      "             ReLU-21          [-1, 128, 20, 20]               0\n",
      "           Conv2d-22          [-1, 128, 20, 20]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 20, 20]             256\n",
      "           Conv2d-24          [-1, 128, 20, 20]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 20, 20]             256\n",
      "             ReLU-26          [-1, 128, 20, 20]               0\n",
      "       BasicBlock-27          [-1, 128, 20, 20]               0\n",
      "           Conv2d-28          [-1, 128, 20, 20]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 20, 20]             256\n",
      "             ReLU-30          [-1, 128, 20, 20]               0\n",
      "           Conv2d-31          [-1, 128, 20, 20]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 20, 20]             256\n",
      "             ReLU-33          [-1, 128, 20, 20]               0\n",
      "       BasicBlock-34          [-1, 128, 20, 20]               0\n",
      "           Conv2d-35          [-1, 256, 10, 10]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 10, 10]             512\n",
      "             ReLU-37          [-1, 256, 10, 10]               0\n",
      "           Conv2d-38          [-1, 256, 10, 10]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 10, 10]             512\n",
      "           Conv2d-40          [-1, 256, 10, 10]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 10, 10]             512\n",
      "             ReLU-42          [-1, 256, 10, 10]               0\n",
      "       BasicBlock-43          [-1, 256, 10, 10]               0\n",
      "           Conv2d-44          [-1, 256, 10, 10]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 10, 10]             512\n",
      "             ReLU-46          [-1, 256, 10, 10]               0\n",
      "           Conv2d-47          [-1, 256, 10, 10]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 10, 10]             512\n",
      "             ReLU-49          [-1, 256, 10, 10]               0\n",
      "       BasicBlock-50          [-1, 256, 10, 10]               0\n",
      "           Conv2d-51            [-1, 512, 5, 5]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-53            [-1, 512, 5, 5]               0\n",
      "           Conv2d-54            [-1, 512, 5, 5]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 5, 5]           1,024\n",
      "           Conv2d-56            [-1, 512, 5, 5]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-58            [-1, 512, 5, 5]               0\n",
      "       BasicBlock-59            [-1, 512, 5, 5]               0\n",
      "           Conv2d-60            [-1, 512, 5, 5]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-62            [-1, 512, 5, 5]               0\n",
      "           Conv2d-63            [-1, 512, 5, 5]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-65            [-1, 512, 5, 5]               0\n",
      "       BasicBlock-66            [-1, 512, 5, 5]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "         Identity-68                  [-1, 512]               0\n",
      "           ResNet-69                  [-1, 512]               0\n",
      "           Linear-70                  [-1, 128]          65,664\n",
      "           Linear-71                 [-1, 4000]         516,000\n",
      "          Softmax-72                 [-1, 4000]               0\n",
      "================================================================\n",
      "Total params: 11,758,176\n",
      "Trainable params: 10,166,368\n",
      "Non-trainable params: 1,591,808\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.29\n",
      "Forward/backward pass size (MB): 32.10\n",
      "Params size (MB): 44.85\n",
      "Estimated Total Size (MB): 77.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Initialize model and load previous weights safely\n",
    "EMB_DIM = 128\n",
    "model = FaceNetSoftmax(num_classes=num_classes, emb_dim=EMB_DIM).to(DEVICE)\n",
    "\n",
    "MODEL_PATH = MODEL_DIR / \"classifier_embed_resnet18_full_cpu.pt\"  # your previous checkpoint\n",
    "if MODEL_PATH.exists():\n",
    "    state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(state, strict=False)  # ignores new softmax layer\n",
    "    print(\"Loaded previous weights (added Softmax for compliance)\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Training from scratch.\")\n",
    "\n",
    "summary(model, input_size=(3, 160, 160), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7bd864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|████████████████████████████████████| 5948/5948 [1:58:22<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.1436 | train_acc=0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|████████████████████████████████████| 5948/5948 [1:59:21<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=1.9249 | train_acc=0.6201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "EPOCHS = 2  # short fine-tune to adjust new softmax layer\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", ncols=90):\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits, probs = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        preds = probs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}: loss={running_loss/len(train_loader):.4f} | train_acc={train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad96233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5040\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Validation accuracy\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        logits, probs = model(imgs)\n",
    "        preds = probs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "val_acc = correct / total if total > 0 else 0\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c9fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model with explicit softmax → d:\\study\\cos30082\\final_asm\\models\\classifier_embed_resnet18_softmax_cpu.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — Save compliant model\n",
    "SAVE_PATH = MODEL_DIR / \"classifier_embed_resnet18_softmax_cpu.pt\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(\"Saved model with explicit softmax →\", SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b449310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Embedding extraction utilities\n",
    "def embed_image(path):\n",
    "    model.eval()\n",
    "    x = val_tf(Image.open(path).convert(\"RGB\")).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        emb = model(x, return_embedding=True).cpu().numpy().squeeze()\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44ace254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different people similarity: -0.0632\n",
      "Same person similarity: 0.8728\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 — Test embedding extraction & similarity\n",
    "from random import choice\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(e1, e2):\n",
    "    return np.dot(e1, e2) / (norm(e1) * norm(e2))\n",
    "\n",
    "# pick random samples\n",
    "cls1, cls2 = choice(train_ds.classes), choice(train_ds.classes)\n",
    "p1 = next((TRAIN_DIR / cls1).glob(\"*\"))\n",
    "p2 = next((TRAIN_DIR / cls2).glob(\"*\"))\n",
    "e1, e2 = embed_image(p1), embed_image(p2)\n",
    "print(f\"Different people similarity: {cosine_similarity(e1, e2):.4f}\")\n",
    "\n",
    "# same person test\n",
    "cls = choice(train_ds.classes)\n",
    "imgs = list((TRAIN_DIR / cls).glob(\"*\"))\n",
    "if len(imgs) >= 2:\n",
    "    e1, e2 = embed_image(imgs[0]), embed_image(imgs[1])\n",
    "    print(f\"Same person similarity: {cosine_similarity(e1, e2):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
