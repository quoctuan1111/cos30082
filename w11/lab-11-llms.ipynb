{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# If you're on Kaggle or Colab, run this once\n!pip install transformers torch --quiet\n\nfrom transformers import pipeline\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T00:56:09.078848Z","iopub.execute_input":"2025-11-13T00:56:09.079155Z","iopub.status.idle":"2025-11-13T00:58:12.317099Z","shell.execute_reply.started":"2025-11-13T00:56:09.079120Z","shell.execute_reply":"2025-11-13T00:58:12.315944Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-13 00:57:47.063792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762995467.335725      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762995467.410375      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Chosen LLM:\n# deepset/roberta-base-squad2 from Hugging Face\n# https://huggingface.co/deepset/roberta-base-squad2\n\nmodel_name = \"deepset/roberta-base-squad2\"\n\n# Create a question-answering pipeline\nqa_pipeline = pipeline(\n    task=\"question-answering\",\n    model=model_name,\n    tokenizer=model_name\n)\n\nprint(\"Q&A model loaded:\", model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T00:58:56.752193Z","iopub.execute_input":"2025-11-13T00:58:56.752647Z","iopub.status.idle":"2025-11-13T00:59:02.882808Z","shell.execute_reply.started":"2025-11-13T00:58:56.752597Z","shell.execute_reply":"2025-11-13T00:59:02.882004Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84881010acd46cd9fa7cfff2dbbbc85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f47022a21f4a1880c38acac09a0418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b30c552913450faeb325522116059d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a8923029404463aaa7b321d6975487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e2a3a21d51e496b9df2b6ca664f7096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15646d80a74c41729861ae26fe832284"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"Q&A model loaded: deepset/roberta-base-squad2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"context_text = \"\"\"\nLarge language models (LLMs) are deep learning models trained on huge amounts of text data.\nThey can understand and generate human-like language and are used for tasks such as\nquestion answering, summarization, translation, and dialogue systems.\n\nThe model used in this notebook, deepset/roberta-base-squad2, is based on RoBERTa and\nfine-tuned on the SQuAD 2.0 dataset for extractive question answering. Given a question\nand a context paragraph, it selects the most relevant span of text as the answer.\n\"\"\"\nprint(context_text[:300] + \"...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T01:02:09.766206Z","iopub.execute_input":"2025-11-13T01:02:09.766753Z","iopub.status.idle":"2025-11-13T01:02:09.773550Z","shell.execute_reply.started":"2025-11-13T01:02:09.766717Z","shell.execute_reply":"2025-11-13T01:02:09.772266Z"}},"outputs":[{"name":"stdout","text":"\nLarge language models (LLMs) are deep learning models trained on huge amounts of text data.\nThey can understand and generate human-like language and are used for tasks such as\nquestion answering, summarization, translation, and dialogue systems.\n\nThe model used in this notebook, deepset/roberta-bas...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def ask_question(question, context=context_text):\n    result = qa_pipeline({\n        \"question\": question,\n        \"context\": context\n    })\n    \n    print(\"Question:\", question)\n    print(\"Raw Answer:\", result[\"answer\"])\n    print(\"Score    :\", round(result[\"score\"], 3))\n    print(\"-\" * 60)\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T01:02:11.710712Z","iopub.execute_input":"2025-11-13T01:02:11.711115Z","iopub.status.idle":"2025-11-13T01:02:11.717018Z","shell.execute_reply.started":"2025-11-13T01:02:11.711083Z","shell.execute_reply":"2025-11-13T01:02:11.715940Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def clean_answer_span(answer):\n    # If the answer contains a list, keep only the first item\n    parts = [p.strip() for p in answer.split(\",\")]\n    return parts[0]\n\n# Example demo\nres = ask_question(\"Name one task that LLMs can perform.\")\nprint(\"Clean Answer:\", clean_answer_span(res[\"answer\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T01:02:21.465154Z","iopub.execute_input":"2025-11-13T01:02:21.465763Z","iopub.status.idle":"2025-11-13T01:02:21.747783Z","shell.execute_reply.started":"2025-11-13T01:02:21.465722Z","shell.execute_reply":"2025-11-13T01:02:21.746908Z"}},"outputs":[{"name":"stdout","text":"Question: Name one task that LLMs can perform.\nRaw Answer: \nquestion answering, summarization, translation, and dialogue systems\nScore    : 0.268\n------------------------------------------------------------\nClean Answer: question answering\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"ask_question(\"What dataset is this model trained on?\")\nask_question(\"What are LLMs trained on?\")\nask_question(\"What can LLMs do?\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T01:02:40.183959Z","iopub.execute_input":"2025-11-13T01:02:40.184304Z","iopub.status.idle":"2025-11-13T01:02:41.024778Z","shell.execute_reply.started":"2025-11-13T01:02:40.184269Z","shell.execute_reply":"2025-11-13T01:02:41.023702Z"}},"outputs":[{"name":"stdout","text":"Question: What dataset is this model trained on?\nRaw Answer: SQuAD 2.0\nScore    : 0.462\n------------------------------------------------------------\nQuestion: What are LLMs trained on?\nRaw Answer: huge amounts of text data\nScore    : 0.794\n------------------------------------------------------------\nQuestion: What can LLMs do?\nRaw Answer: understand and generate human-like language\nScore    : 0.565\n------------------------------------------------------------\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'score': 0.5645219087600708,\n 'start': 102,\n 'end': 145,\n 'answer': 'understand and generate human-like language'}"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"print(\"Type your questions (type 'quit' to exit)\\n\")\n\nwhile True:\n    q = input(\"Your question: \")\n    if q.lower() in [\"quit\", \"exit\"]:\n        break\n    if q.strip():\n        ask_question(q)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T01:03:59.062449Z","iopub.execute_input":"2025-11-13T01:03:59.062858Z","iopub.status.idle":"2025-11-13T01:05:31.312639Z","shell.execute_reply.started":"2025-11-13T01:03:59.062824Z","shell.execute_reply":"2025-11-13T01:05:31.311286Z"}},"outputs":[{"name":"stdout","text":"Type your questions (type 'quit' to exit)\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your question:  what can you do?\n"},{"name":"stdout","text":"Question: what can you do?\nRaw Answer: understand and generate human-like language\nScore    : 0.099\n------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your question:  how can u give an answer?\n"},{"name":"stdout","text":"Question: how can u give an answer?\nRaw Answer: a question\nand a context paragraph\nScore    : 0.132\n------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your question:  quit\n"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}